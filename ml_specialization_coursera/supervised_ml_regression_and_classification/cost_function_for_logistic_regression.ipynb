{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost funtion for logistic regression\n",
    "\n",
    "A cost function, also known as a loss function or objective function, is a mathematical function used in machine learning and optimization to measure the error or \"cost\" of a model's predictions compared to the actual target values. The goal of training a machine learning model is to minimize this cost function, thereby improving the model's accuracy.\n",
    "\n",
    "# Key Points:\n",
    "- Purpose: The cost function quantifies the difference between the predicted values and the actual values. It provides a measure of how well the model is performing.\n",
    "- Optimization: During training, optimization algorithms (e.g., Gradient Descent) adjust the model parameters to minimize the cost function.\n",
    "- Types of Cost Functions: Different types of cost functions are used depending on the type of machine learning problem (e.g., regression, classification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array_like\n",
    "        A scalar or numpy array of any size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "     g : array_like\n",
    "         sigmoid(z)\n",
    "    \"\"\"\n",
    "    z = np.clip( z, -500, 500 )           # protect against overflow\n",
    "    g = 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36686678640551745\n"
     ]
    }
   ],
   "source": [
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "result = compute_cost_logistic(X_train, y_train, w_tmp, b_tmp)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for b = -3 :  0.36686678640551745\n",
      "Cost for b = -4 :  0.5036808636748461\n"
     ]
    }
   ],
   "source": [
    "w_array1 = np.array([1,1])\n",
    "b_1 = -3\n",
    "w_array2 = np.array([1,1])\n",
    "b_2 = -4\n",
    "\n",
    "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
    "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Logistic Regression\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning models, including Logistic Regression. In the context of Logistic Regression, Gradient Descent is used to find the optimal parameters (weights) that minimize the cost function, which measures the difference between the predicted probabilities and the actual class labels.\n",
    "\n",
    "Key Concepts\n",
    "- Logistic Regression: A classification algorithm used to predict the probability of a binary outcome (0 or 1). The model uses the logistic function (sigmoid function) to map the linear combination of input features to a probability.\n",
    "\n",
    "- Cost Function: For Logistic Regression, the cost function is typically the Cross-Entropy Loss (Log Loss), which measures the performance of the classification model.\n",
    "\n",
    "- Gradient Descent: An iterative optimization algorithm used to minimize the cost function by updating the model parameters in the direction of the steepest descent (negative gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlc = dict(dlblue = '#0096ff', dlorange = '#FF9300', dldarkred='#C00000', dlmagenta='#FF40FF', dlpurple='#7030A0')\n",
    "dlblue = '#0096ff'; dlorange = '#FF9300'; dldarkred='#C00000'; dlmagenta='#FF40FF'; dlpurple='#7030A0'\n",
    "dlcolors = [dlblue, dlorange, dldarkred, dlmagenta, dlpurple]\n",
    "\n",
    "def plot_data(X, y, ax, pos_label=\"y=1\", neg_label=\"y=0\", s=80, loc='best' ):\n",
    "    \"\"\" plots logistic data with two axis \"\"\"\n",
    "    # Find Indices of Positive and Negative Examples\n",
    "    pos = y == 1\n",
    "    neg = y == 0\n",
    "    pos = pos.reshape(-1,)  #work with 1D or 1D y vectors\n",
    "    neg = neg.reshape(-1,)\n",
    "\n",
    "    # Plot examples\n",
    "    ax.scatter(X[pos, 0], X[pos, 1], marker='x', s=s, c = 'red', label=pos_label)\n",
    "    ax.scatter(X[neg, 0], X[neg, 1], marker='o', s=s, label=neg_label, facecolors='none', edgecolors=dlblue, lw=3)\n",
    "    ax.legend(loc=loc)\n",
    "\n",
    "    ax.figure.canvas.toolbar_visible = False\n",
    "    ax.figure.canvas.header_visible = False\n",
    "    ax.figure.canvas.footer_visible = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Implementation\n",
    "The gradient descent algorithm implementation has two components: \n",
    "- The loop implementing equation (1) above. This is `gradient_descent` below and is generally provided to you in optional and practice labs.\n",
    "- The calculation of the current gradient, equations (2,3) above. This is `compute_gradient_logistic` below. You will be asked to implement this week's practice lab.\n",
    "\n",
    "#### Calculating the Gradient, Code Description\n",
    "Implements equation (2),(3) above for all $w_j$ and $b$.\n",
    "There are many ways to implement this. Outlined below is this:\n",
    "- initialize variables to accumulate `dj_dw` and `dj_db`\n",
    "- for each example\n",
    "    - calculate the error for that example $g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - \\mathbf{y}^{(i)}$\n",
    "    - for each input value $x_{j}^{(i)}$ in this example,  \n",
    "        - multiply the error by the input  $x_{j}^{(i)}$, and add to the corresponding element of `dj_dw`. (equation 2 above)\n",
    "    - add the error to `dj_db` (equation 3 above)\n",
    "\n",
    "- divide `dj_db` and `dj_dw` by total number of examples (m)\n",
    "- note that $\\mathbf{x}^{(i)}$ in numpy `X[i,:]` or `X[i]`  and $x_{j}^{(i)}$ is `X[i,j]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.49861806546328574\n",
      "dj_dw: [0.498333393278696, 0.49883942983996693]\n"
     ]
    }
   ],
   "source": [
    "## Check the compute_gradient_logistic function\n",
    "X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2.,3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\" )\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic(X, y, w, b) )\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history         #return final w,b and J history for graphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.684610468560574   \n",
      "Iteration 1000: Cost 0.1590977666870457   \n",
      "Iteration 2000: Cost 0.08460064176930078   \n",
      "Iteration 3000: Cost 0.05705327279402531   \n",
      "Iteration 4000: Cost 0.04290759421682   \n",
      "Iteration 5000: Cost 0.03433847729884557   \n",
      "Iteration 6000: Cost 0.02860379802212006   \n",
      "Iteration 7000: Cost 0.02450156960879306   \n",
      "Iteration 8000: Cost 0.02142370332569295   \n",
      "Iteration 9000: Cost 0.019030137124109114   \n",
      "\n",
      "updated parameters: w:[5.28123029 5.07815608], b:-14.222409982019837\n"
     ]
    }
   ],
   "source": [
    "w_tmp  = np.zeros_like(X_train[0])\n",
    "b_tmp  = 0.\n",
    "alph = 0.1\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sckykit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-3 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-3 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-3 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-3 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-3 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-3 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-3 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-3 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-3 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install scikit-learn package\n",
    "#%pip install scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# The code below imports the logistic regression model from scikit-learn. You can fit this model on the training data by calling fit function.\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on training set: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = lr_model.predict(X)\n",
    "\n",
    "print(\"Prediction on training set:\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "print(\"Accuracy on training set:\", lr_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfiting\n",
    "\n",
    "Overfitting in machine learning refers to a model that learns the training data too well, capturing noise and details that do not generalize to new, unseen data. This results in a model that performs well on the training data but poorly on the test data or any new data.\n",
    "\n",
    "Key Points:\n",
    "- High Variance: Overfitting is characterized by high variance, meaning the model's predictions vary significantly with different datasets.\n",
    "- Complex Models: Overfitting often occurs with complex models that have too many parameters relative to the amount of training data.\n",
    "- Poor Generalization: An overfitted model fails to generalize from the training data to new data, leading to poor performance on validation or test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization \n",
    "\n",
    "Regularization in machine learning refers to techniques used to prevent overfitting by adding a penalty to the model's complexity. It helps to improve the generalization of the model by discouraging it from fitting the noise in the training data. Regularization techniques add a regularization term to the cost function, which penalizes large coefficients or weights.\n",
    "\n",
    "Key Points:\n",
    "- Prevent Overfitting: Regularization helps to prevent overfitting by making the model simpler and more generalizable.\n",
    "- Penalty Term: A regularization term is added to the cost function to penalize large weights or coefficients.\n",
    "- Types of Regularization: The most common types of regularization are L1 (Lasso) and L2 (Ridge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m  = X.shape[0]\n",
    "    n  = len(w)\n",
    "    cost = 0.\n",
    "    # Comprute cost\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b                                   #(n,)(n,)=scalar, see np.dot\n",
    "        cost = cost + (f_wb_i - y[i])**2                               #scalar             \n",
    "    cost = cost / (2 * m)                                              #scalar  \n",
    " \n",
    " \n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost                                                  #scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.07917239320214275\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function for regularized linear regression\n",
    "def compute_gradient_linear_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "      \n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "#----Compute gradient\n",
    "    for i in range(m):                             \n",
    "        err = (np.dot(X[i], w) + b) - y[i]                 \n",
    "        for j in range(n):                         \n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]               \n",
    "        dj_db = dj_db + err                        \n",
    "    dj_dw = dj_dw / m                                \n",
    "    dj_db = dj_db / m   \n",
    "# ----------------------------\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.6648774569425726\n",
      "Regularized dj_dw:\n",
      " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp =  compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient function for regularized logistic regression\n",
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    ### START CODE HERE ### \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        # Calculate f_wb (exactly how you did it in the compute_cost function above)\n",
    "        z_wb = 0\n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "        # Add the corresponding term to z_wb\n",
    "            z_wb_ij = X[i, j] * w[j]\n",
    "            z_wb += z_wb_ij\n",
    "\n",
    "        # Add bias term \n",
    "        z_wb += b\n",
    "\n",
    "        # Calculate the prediction from the model\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = 1 if f_wb >= 0.5 else 0\n",
    "        \n",
    "    ### END CODE HERE ### \n",
    "    return p\n",
    "\n",
    "#Compute accuracy on our training set\n",
    "# p = predict(X_train, w,b)\n",
    "# print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mlini2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
